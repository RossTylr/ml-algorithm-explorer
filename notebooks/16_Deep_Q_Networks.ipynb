{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b824336a-3aed-4e07-ba02-d92a760a89ba",
   "metadata": {},
   "source": [
    "# 16. Deep Q-Networks (DQN)  \n",
    "**Author**: Your Name  \n",
    "**Date**: June 9, 2025  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "DQN combines **Q-Learning** with **deep neural networks** to handle environments with **large or continuous state spaces**.\n",
    "\n",
    "- **Type**: Model-Free, Off-Policy, Value-Based RL  \n",
    "- **Task**: Policy learning via function approximation  \n",
    "- **Goal**: Approximate Q(s, a) using a neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bd8a8-fcb5-453f-89f0-7e1506e7c38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b6078-b34a-4ae2-ae16-39479120423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow logging and disable GPU if not needed\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'            # Suppress INFO and WARNING logs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'           # Prevent GPU detection to speed up CPU-only workflows\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Environment and simulation\n",
    "import gymnasium as gym\n",
    "\n",
    "# Machine learning (TensorFlow/Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Plotting configuration\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e28362-3102-4fc9-b8e7-bcb6a54b9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "action_space_size = env.action_space.n\n",
    "\n",
    "print(\"State Space Size:\", state_space_size)\n",
    "print(\"Action Space Size:\", action_space_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2e898-eda4-45c3-9337-65bb03a9fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.policy_network = self._build_model()\n",
    "        self.target_network = self._build_model()\n",
    "        self.update_target_network()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.policy_network.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.policy_network.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward if done else reward + self.gamma * np.amax(self.target_network.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.policy_network.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.policy_network.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "agent = DQNAgent(state_space_size, action_space_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e8ebd-3e94-464d-8c61-a77ec70379fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 500\n",
    "batch_size = 32\n",
    "update_target_every = 10\n",
    "scores = []\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "for e in range(total_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_space_size])\n",
    "\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_space_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            scores.append(time + 1)\n",
    "            print(f\"Episode {e+1}/{total_episodes}, Score: {time+1}, Epsilon: {agent.epsilon:.2f}\")\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    if e % update_target_every == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d75ce-2f65-4594-8ad1-4675cc744935",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_scores = pd.Series(scores).rolling(100).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(scores, alpha=0.5, label='Episode Score')\n",
    "plt.plot(moving_avg_scores, linewidth=2, label='100-Episode Moving Average')\n",
    "plt.title('DQN Training Performance on CartPole-v1')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Score (Time Steps)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abd4b6-9c0b-49b9-aeda-1bb76bdec70b",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- DQNs use neural networks to generalise Q-values over continuous/high-dimensional states.\n",
    "- Experience Replay and Fixed Q-Targets are critical for training stability.\n",
    "- A foundational algorithm behind many advanced RL methods.\n",
    "- Still limited in continuous **action** spaces (policy gradient methods required).\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- DeepMind (2015): *Human-level control through deep reinforcement learning*\n",
    "- Sutton & Barto: *Reinforcement Learning: An Introduction*\n",
    "- [TensorFlow DQN Tutorial](https://www.tensorflow.org/tutorials/reinforcement_learning/intro)\n",
    "- [StatQuest: Deep Q-Learning Explained](https://www.youtube.com/watch?v=79pmNdyxEGo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50473155-d85a-4c21-80b9-7977f9c8046f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3c72b-603b-4ede-b131-1057a04bc4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b6503-7ebc-470c-a21e-4882c68eaea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde05936-0fc1-4811-9bbb-7be17e203878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17609d24-5fb6-4ac1-987f-7c5831b0c85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
